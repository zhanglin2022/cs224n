## Written: Understanding word2vec (26 points)
(a) (3 points) Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\hat{y}$; i.e., show that<br>
$$
-\sum_{w\in \mathrm{Vocab}}{y_w}\log \left( \hat{y}_w \right) =-\log \left( \hat{y}_o \right) \ \left( 3 \right) 
$$
Your answer should be one line

